Visual implants are intended to produce an artificial vision leading to some levels of functional vision restoration. Due to the limited number of microelectrodes of existing visual system stimulator, the artificial vision has very low resolution. Many researchers have worked on improving the artificial vision created with low resolution implants by using image processing and machine vision algorithms. Users express dissatisfaction with the Retinal Prosthesis System due to the low resolution of phosphine images, highlighting the critical need for focused research to enhance visual clarity and improve overall user satisfaction. This project proposes a simulation of the artificial vision in which the information synthesized by the system to the visually impaired user using a visual implants generated low resolution phosphene image. By employing Vision Transformer (ViT), it extracts valuable information about individuals surrounding the visually impaired user, such as their count, familiarity, gender, estimated ages, facial emotions, surrounding objects and approximate distances. This data, derived from camera frames on the user's glasses, is utilized to generate signals fed into a visual stimulator, presenting a promising approach to enrich the visual experience for individuals with visual impairments. For each feature, an appropriate algorithm is selected based on its accuracy and time complexity to enable affordable real-time implementations in an autonomous portable system. The proposed system conveys important information about the people around a visually impaired person through audio and make that person more comfortable to communicate. Thus, it can be considered for some next generation visual implant systems.
